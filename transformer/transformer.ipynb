{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer.ipynb","provenance":[],"mount_file_id":"1uzHizG-a7RzHpH4A7mTJQ-4Gs673yzAk","authorship_tag":"ABX9TyPuWBFh87NQCMbT/uGVlROb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Reference [labml.ai](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/940b3c01fc87c9487ad9478eec09f0167e177e2a/labml_nn/transformers/positional_encoding.py#L1)"],"metadata":{"id":"b3YAsl94TTtj"}},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"VrbVt2R_MKUu","executionInfo":{"status":"ok","timestamp":1659924191115,"user_tz":-540,"elapsed":526,"user":{"displayName":"HyeonGeun Yoon","userId":"03668824595815287272"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/data_science_project/deeplearning_paper/transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0s6yt0MHMXi-","executionInfo":{"status":"ok","timestamp":1659924233943,"user_tz":-540,"elapsed":512,"user":{"displayName":"HyeonGeun Yoon","userId":"03668824595815287272"}},"outputId":"c3d1bdfe-a111-46d2-f5f0-eab0afacb000"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data_science_project/deeplearning_paper/transformer\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9Md_0iM6pqIi","executionInfo":{"status":"ok","timestamp":1659926041459,"user_tz":-540,"elapsed":416,"user":{"displayName":"HyeonGeun Yoon","userId":"03668824595815287272"}}},"outputs":[],"source":["import math\n","\n","import torch\n","import torch.nn as nn\n","\n","from utils.positional_encoding import get_positional_encoding\n","from utils.mha import MultiHeadAttention"]},{"cell_type":"markdown","source":["# Positional Encoding"],"metadata":{"id":"YGzqAc9LQ9-0"}},{"cell_type":"markdown","source":["## Fixed positional encoding"],"metadata":{"id":"fQ1WlSMTRBBv"}},{"cell_type":"code","source":["class EmbddingWithPositionalEncoding(nn.Module):\n","    \"\"\"\n","    논문에 나온 방식, 고정된 positinal encoding\n","    \"\"\"\n","\n","    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n","        super().__init__()\n","        self.linear = nn.Embedding(n_vocab, d_model)\n","        self.d_model = d_model\n","        self.register_buffer('positional_encodings', get_positional_encoding(d_model, max_len))\n","\n","    def forward(self, x: torch.Tensor):\n","        pe = self.positional_encodings[:x.shape[0].reguires_grad_(False)]\n","        return self.linear(x) * math.sqrt(self.d_model) + pe"],"metadata":{"id":"2ZEsjYS2MJvv","executionInfo":{"status":"ok","timestamp":1659925377233,"user_tz":-540,"elapsed":358,"user":{"displayName":"HyeonGeun Yoon","userId":"03668824595815287272"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Paremeterized positional encoding"],"metadata":{"id":"9MAGYxFlRDXg"}},{"cell_type":"code","source":["class EmbddingsWithLearnedPositionalEncoding(nn.Module):\n","    \"\"\"\n","    훈련 가능한 parameter로 positional encoding 수행\n","    \"\"\"\n","    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n","        super().__init__()\n","        self.linear = nn.Embedding(n_vocab, d_model)\n","        self.d_model = d_model\n","        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True) # (S, B, E)\n","    \n","\n","    def forward(self, x: torch.Tensor):\n","        pe = self.positional_encodings[:x.shape[0]]\n","        return self.linear(x) * math.sqrt(self.d_model) + pe"],"metadata":{"id":"i98PBRYqNq5S","executionInfo":{"status":"ok","timestamp":1659925377539,"user_tz":-540,"elapsed":1,"user":{"displayName":"HyeonGeun Yoon","userId":"03668824595815287272"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# TransformerLayer"],"metadata":{"id":"chXS9MuyOW7Y"}},{"cell_type":"code","source":["from torch.nn.modules.activation import MultiheadAttention\n","class TransformerLayer(nn.Module):\n","    def __init__(self, *,\n","                 d_model: int, \n","                 self_attn: MultiHeadAttention, # self attentionmodule\n","                 src_attn: MultiheadAttention = None, # source attention module (디코더일 때 사용)\n","                 feed_forward: FeedForward, # feedforward 네트워크\n","                 dropout_prob: float\n","                 ):\n","        \n","        super"],"metadata":{"id":"4DLQ1gMaRv7m"},"execution_count":null,"outputs":[]}]}